"""
LIBRARY SPECIFICATION: google-genai (v1.52.0)
TYPE: Unified SDK for Gemini API (Google AI Studio) and Vertex AI
"""

# 1. IMPORTS & SETUP
from google import genai
from google.genai import types

# Initialize Client (Google AI Studio - API Key)
client = genai.Client(api_key="YOUR_API_KEY")

# Initialize Client (Vertex AI - OAuth/ADC)
client = genai.Client(
    vertexai=True,
    project="your-project-id",
    location="us-central1"
)

# 2. CORE INFERENCE (Synchronous)
# Method: client.models.generate_content
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="Explain quantum computing.",
    config=types.GenerateContentConfig(
        temperature=0.7,
        max_output_tokens=500,
        system_instruction="You are a helpful expert."
    )
)
print(response.text)

# 3. STREAMING
# Method: client.models.generate_content_stream
for chunk in client.models.generate_content_stream(
    model="gemini-2.0-flash",
    contents="Write a long story..."
):
    print(chunk.text, end="")

# 4. ASYNCHRONOUS USAGE
# Access async methods via the `.aio` property on the client
async def main():
    # Option A: Use existing client.aio
    response = await client.aio.models.generate_content(
        model="gemini-2.0-flash",
        contents="Hello world"
    )
    
    # Option B: Async Streaming
    async for chunk in client.aio.models.generate_content_stream(
        model="gemini-2.0-flash",
        contents="Hello world"
    ):
        print(chunk.text)

# 5. CHAT / MULTI-TURN
# The SDK handles chat via the contents list (History + New Message)
chat = client.chats.create(model="gemini-2.0-flash")
response = chat.send_message("Hello!")
response = chat.send_message("Tell me more.")

# Alternatively, manual history management in generate_content:
contents = [
    types.Content(role="user", parts=[types.Part.from_text("Hi")]),
    types.Content(role="model", parts=[types.Part.from_text("Hello!")]),
    types.Content(role="user", parts=[types.Part.from_text("How are you?")])
]

# 6. CONFIGURATION TYPES
# Common configurations used in `config=` parameter
config = types.GenerateContentConfig(
    temperature=0.5,
    top_p=0.95,
    top_k=40,
    candidate_count=1,
    stop_sequences=["END"],
    response_mime_type="application/json" # For JSON mode
)

# 7. UTILITIES
# File Uploading (if using Google AI Studio)
file_ref = client.files.upload(file="path/to/image.png")
# Use in content: contents=[file_ref, "Describe this image"]

# 8. Example for a script be called from authenticated Client

```python
"""
LIBRARY: google-genai (v1.52.0)
DOCS: https://googleapis.github.io/python-genai/
TYPE: Unified SDK for Gemini (Google AI Studio) & Vertex AI
"""

# 1. SETUP & CLIENT
from google import genai
from google.genai import types

# Initialize: Google AI Studio (API Key)
client = genai.Client(api_key="GEMINI_API_KEY")

# Initialize: Vertex AI (ADC / OAuth)
client = genai.Client(
    vertexai=True,
    project="my-project-id",
    location="us-central1"
)

# 2. GENERATE CONTENT (Synchronous)
# Primary method for text/multimodal generation
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="Explain how AI works.",
    config=types.GenerateContentConfig(
        temperature=0.7,
        top_p=0.95,
        max_output_tokens=500,
        response_mime_type="application/json", # Structured output
        system_instruction="You are a coding expert."
    )
)
print(response.text)
# Usage of Pydantic models is supported for configuration.

# 3. MULTIMODAL / PARTS
# The SDK uses a explicit `types.Part` system for complex inputs
image_part = types.Part.from_uri(
    file_uri="gs://bucket/image.jpg",
    mime_type="image/jpeg"
)
text_part = types.Part.from_text(text="Describe this image.")

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=[types.Content(role="user", parts=[text_part, image_part])]
)

# 4. STREAMING
# Returns an iterable of chunks
for chunk in client.models.generate_content_stream(
    model="gemini-2.0-flash",
    contents="Write a long essay."
):
    print(chunk.text, end="")

# 5. ASYNC (AIO)
# Access async methods via the `.aio` property
async def generate():
    response = await client.aio.models.generate_content(
        model="gemini-2.0-flash",
        contents="Hello world"
    )
    print(response.text)

# 6. CHATS (Stateful)
# Manages history automatically
chat = client.chats.create(
    model="gemini-2.0-flash",
    config=types.GenerateContentConfig(temperature=0.5)
)
response = chat.send_message("Hello!")
print(response.text)

# 7. FUNCTION CALLING (Automatic)
# Pass Python functions directly to `tools`
def get_weather(location: str):
    """Returns weather for a location."""
    return "Sunny"

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="What's the weather in Paris?",
    config=types.GenerateContentConfig(
        tools=[get_weather] # Auto-execution is enabled by default
    )
)
print(response.text)

# 8. FILE UPLOADS (Google AI Studio Only)
# Helper for uploading local files for the model to use
file_ref = client.files.upload(file="path/to/doc.pdf")
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=[file_ref, "Summarize this document."]
)
```